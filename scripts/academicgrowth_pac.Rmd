---
title: "Longitudinal Analysis of Academic Growth Patterns at Morningside Academy:
  A Five-Year Assessment Study"
author: "Saratessa Palos, Will Arangelov, Marcus Cumberbatch"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
#bibliography: references.bib
#csl: apa.csl
abstract: |
This study evaluates the Morningside Model of Generative Instruction using standardized assessment data from four school years (2008–2009, 2015–2016, 2016–2017, and 2023–2024). Repeated measures ANOVA and mixed-effects models revealed significant academic growth, with math and writing outperforming reading. Growth peaked during 2015–2017. Findings confirm MMGI’s effectiveness in addressing educational deficits. Results emphasize the importance of evidence-based instruction and consistent funding for sustaining educational advancements.
---

```{r setup, include=FALSE}
# Include libraries here
library(here)
library(readr)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(car)
library(lme4)
library(lmerTest)
library(emmeans)

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction
The Morningside Model of Generative Instruction (MMGI) is a research based, empirically validated educational model shown to be extremely effective in developing competent learners. Morningside Academy, established in Seattle, Washington, utilizes the MMGI approach to help elementary and middle school children who are one or more years behind expected grade level to catch up and excel academically. 

The school guarantees that students will gain at least two grade levels in their area of greatest deficit within one academic year. Unlike traditional special education models that emphasize compensatory strategies, Morningside directly addresses student deficits through performance-based placement and systematic instruction. 

This study aims to validate these educational approaches through analysis of standardized assessment data collected over four academic years. The use of multiple assessment systems (WJ, ITBS, and IA) provides complementary measures of student achievement across reading, writing, and mathematics domains. Understanding patterns of academic growth is crucial for evaluating educational effectiveness and informing instructional practices.

To better assess the efficacy of the MMGI, we analyzed 1) patterns of academic growth across different assessment types (math, reading, and writing) between fall and spring testing periods, and 2) how patterns for these domains vary across years analyzed.

# Methods
This study analyzed student assessment data from five academic years at Morningside Academy (2008-2009, 2015-2016, 2016-2017, and 2023-2024). Data cleaning and analysis were conducted using R statistical software with tidyverse, readxl, corrplot, viridis, and gridExtra packages. For each academic year, fall and spring assessment scores were processed separately. All datasets required standardization of ceiling scores (converting ">12.9" to "12.9" and "<3.0" to "3.0"), and numeric conversion of assessment scores. Growth scores were calculated by subtracting fall scores from spring scores across multiple academic measures including Reading Comprehension (RC), Letter-Word Identification (LWID), Word Attack (WA), Reading Fluency (RF), Writing Fluency (WF), Writing Samples (WS), Calculation (CALC), and Math Fluency (MF). Student names were standardized across datasets using a custom function to handle different name formats (LastName, FirstName vs. FirstName LastName). The datasets were then merged using bind_rows after selecting standardized names, year identifiers, and growth measures.

# Results

```{r read-data, include=FALSE}
here::here()
data <- read.csv("~/Desktop/updated_repo/data/updated.csv")
```

```{r datasets, include=FALSE}
#'analysis_data' (clean version of 'data' with 10 variables, and without years 2009-2010) 

#removing rows for 2009-2010 data
analysis_data <- data %>%
  filter(year != "2009-2010")

#removing columns WJID_growth to Math_growth
columns_to_remove <- which(names(analysis_data) == "WJ_LWID_growth"):which(names(analysis_data) == "Math_growth")
analysis_data <- analysis_data[, -columns_to_remove]

#Shifting 'ID' column to left of 'year'
analysis_data <- analysis_data[, c("id", "year", setdiff(names(analysis_data), c("id", "year")))]
```

```{r analysis-avgs, include=FALSE}
#Adding merged means columns (across academic domain measures) to 'data2' frame, for secondary research question
analysis_avgs <- analysis_data

#Reading
analysis_avgs$reading <- (analysis_avgs$RC_growth + analysis_avgs$LWID_growth + analysis_avgs$RF_growth) / 3
analysis_avgs$reading <- round(analysis_avgs$reading, 2)

#Writing
analysis_avgs$writing <- (analysis_avgs$WA_growth + analysis_avgs$WF_growth + analysis_avgs$WS_growth) / 3
analysis_avgs$writing <- round(analysis_avgs$writing, 2)

#Math
analysis_avgs$math <- (analysis_avgs$CALC_growth + analysis_avgs$MF_growth) / 2
analysis_avgs$math <- round(analysis_avgs$math, 2)

#Writing .csv file for data frame
# write_csv(averages_data2, here::here("Desktop", "REORG'D FINAL", "data", "averages_data2.csv"))
```

```{r grand-means, include=FALSE}
#Obtaining grand means for reading, writing, and math measures
mean_reading <- mean(analysis_avgs$reading, na.rm = TRUE)
mean_reading <- round(mean_reading, 2)
mean_reading

mean_writing <- mean(analysis_avgs$writing, na.rm = TRUE)
mean_writing <- round(mean_writing, 2)

mean_math <- mean(analysis_avgs$math, na.rm = TRUE)
mean_math <- round(mean_math, 2)
```

```{r domain-avgs, include=FALSE}
#Obtaining academic domain averages by year
#Reading
reading_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_reading = round(mean(reading, na.rm = TRUE), 2))
reading_by_year

#Writing
writing_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_writing = round(mean(writing, na.rm = TRUE), 2))
writing_by_year

#Math
math_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_math = round(mean(math, na.rm = TRUE), 2))
math_by_year

#Merged table for academic growth averages, by year
merged_means <- left_join(reading_by_year, writing_by_year, math_by_year, by = "year")
merged_means <- left_join(merged_means, math_by_year, by = "year")
merged_means
```

Our analytic strategy first involved obtaining aggregate scores of different growth measure types across participants. Reading comprehension, letter-word identification, and reading fluency measures were coalesced into composite scores. Next, we repeated this process for writing, by combining writing fluency, writing samples, and word attack growth scores. Finally, we combined the two assessment measures from mathematics, CALC as well as math fluency, into averaged scores. We then obtained grand means for each of the three academic domains (`Reading` , `Writing` , and `Math`). Refer to table 1 for a breakdown of these means.

```{r avgs-table, echo=FALSE}
#Kable table of merged means
merged_means %>%
  kbl(
    col.names = c("Year", "Mean Reading", "Mean Writing", "Mean Math"),
    caption = "Table 1: Average Growth by Academic Domain",
    booktabs = TRUE
  ) %>%
  kable_styling(
    full_width = FALSE, 
    position = "center"
  ) %>%
  add_header_above(c(" " = 1, "Academic Growth Averages" = 3)) %>%
  footnote(
    general = "Note: NaN indicates missing data for that academic year.",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```

In addition to means analyses, we conducted statistical tests to observe mean comparisons of academic domains, on aggregate. These tests allowed us to account for non-unique participant identification numbers within the current data, and further enabled us to observe potential differences in growth patterns across various academic years.

### Research Question 1

#### *Analytic Strategy*

A repeated measures analysis of variance (ANOVA) was conducted via the `aov` function in R to quantify patterns of academic growth across different assessment measure types. Individual measure types were aggregated by domain across participants to create three, composite academic domains of math, reading, and writing.. The repeated measures approach was utilized because growth scores were measured several times for each student across several assessment types within a given year (as well as during multiple years). Thus, we treated `assessment_type` as a within-subject factor. Since the data included non-unique student IDs, the model accounted for repeated measures per student. Post-hoc pairwise comparisons with Bonferroni correction were conducted to determine specific differences between the three assessment types, to account for uneven sample sizes within `year`.

#### *Results*

```{r rq1-results, include=FALSE}
# Repeated Measures ANOVA: testing
# Transforming data to long format
analysis_long <- analysis_avgs %>% 
  pivot_longer(cols = c(reading, writing, math),
               names_to = "Assessment_Type",
               values_to = "Growth_Score")

#Removing extraneous columns
analysis_long <- analysis_long %>% 
  select(-c(RC_growth:MF_growth))

#Renaming measures
analysis_long <- analysis_long %>%
  mutate(
    Assessment_Type = case_when(
      Assessment_Type == "math" ~ "Math",
      Assessment_Type == "reading" ~ "Reading",
      Assessment_Type == "writing" ~ "Writing",
      TRUE ~ Assessment_Type
    )
  )

# Variable Structure Check
str(analysis_long)

# Changing variables from characters to factors
analysis_long$id <- as.factor(analysis_long$id)
analysis_long$Assessment_Type <- as.factor(analysis_long$Assessment_Type)
analysis_long$year <- as.factor(analysis_long$year)

# Running a repeated Measures ANOVA to analyze RQ1a
aov_analysis <- aov(Growth_Score ~ Assessment_Type + year + Error(id/(Assessment_Type * year)), 
                  data = analysis_long)

#Transforming data into wide format
analysis_wide <- analysis_long %>% 
  pivot_wider(names_from = Assessment_Type, values_from = Growth_Score)
```

```{r inline-anova, include=FALSE}
# For inline code:
f1 <- 5.81
df1 <- 2
df2 <- 259
pval1 <- .003

f2 <- 9.27
df3 <- 3
df4 <- 259
pval2 <- .001

f3 <- 0.62
df5 <- 1
df6 <- 402
pval3 <- .43

# *F*(`r df1`, `r df2`) = `r round(f1, 2)`, *p* < `r ifelse(p_val1 < 0.001, ".001", format(p_val1, digits=3))`

# *F*(`r df3`, `r df4`) = `r round(f2, 2)`, *p* < `r ifelse(p_val2 < 0.001, ".001", format(p_val2, digits=3))`

# *F*(`r df5`, `r df6`) = `r round(f3, 2)`, *p* < `r ifelse(p_val3 < 0.001, ".001", format(p_val3, digits=3))`
```

A repeated measures ANOVA was conducted to examine the effect of Assessment Type (reading, writing, and math) on academic growth scores. Results showed a significant main effect of `assessment_type` , *F*(`r df1`, `r df2`) = `r round(f1, 2)`, *p* \< `r ifelse(pval1 < 0.001, ".001", format(pval1, digits=3))`, indicating that growth scores varied across assessment types. Additionally, there was a significant main effect of `year` , *F*(`r df3`, `r df4`) = `r round(f2, 2)`, *p* \< `r ifelse(pval2 < 0.001, ".001", format(pval2, digits=3))`, suggesting that academic growth differed across academic years. However, the interaction between `assessment_type` and `year` was not significant, *F*(`r df5`, `r df6`) = `r round(f3, 2)`, *p* \< `r ifelse(pval3 < 0.001, ".001", format(pval3, digits=3))`, indicating that growth patterns across assessments were consistent over time.

Post-hoc pairwise comparison tests using a Bonferroni correction revealed that growth in `Reading` was significantly lower than in `Math` (*M*difference = -0.92, *p* = 0.015) and `Writing` (*M*difference = -1.10, *p* = 0.003). No significant differences emerged between `Math` and `Writing` (*p* = 1.00), however. These comparisons suggest that students experienced the least academic growth in reading, while growth in math and writing was statistically similar (refer to the ANOVA model output below for full transparency). Results are shown in Fig. 1.

```{r rq1-output, echo=FALSE}
# ANOVA summary (PRINT)
summary(aov_analysis)

# ANOVA Post Hoc: Bonferroni (uneven groups)...(PRINT)
pairwise.t.test(analysis_long$Growth_Score, analysis_long$Assessment_Type, 
                p.adjust.method = "bonferroni")
```

```{r rq1-visual, echo=FALSE}
#Bar plot
anova_analysis <- analysis_long %>%
  group_by(Assessment_Type) %>%
  summarize(
    Mean_Growth = mean(Growth_Score, na.rm = TRUE),
    SD = sd(Growth_Score, na.rm = TRUE),
    N = n(),
    SE = SD / sqrt(N)
  )

ggplot(anova_analysis, aes(x = Assessment_Type, y = Mean_Growth)) +
  geom_bar(stat = "identity", color = "black", fill = "lightgrey", width = 0.6) +
  geom_errorbar(aes(ymin = Mean_Growth - SE, ymax = Mean_Growth + SE), width = 0.1) +
  labs(
    title = "Fig. 1",
    subtitle = "Academic Growth by Assessment Type",
    x = "Assessment Type",
    y = "Mean Growth Score"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

### Research Question 2

#### *Analytic Strategy*

To examine how academic growth varied across different academic years, a linear mixed-effects model (LMM) was conducted, using the `lmer` function within R, in addition to `car` , `lme4` , and `lmertest` for relevant statistics of significance in the output. Academic `year` was utilized as a fixed effect, with student ID included as a random effect to account for repeated measurements, due to certain students appearing in the data set multiple times across years. This method allowed us to model within-student variability while estimating the average effect of academic year on academic growth in reading, writing, and math. The LMM was fit using restricted maximum likelihood (REML) for unbiased estimates of fixed and random effects.

### Results

```{r rq2-lmm, include=FALSE}
#LMM for reading (PRINT MODEL OUTPUTS)
#LMM
#Checking variable structure
str(analysis_avgs)

#Changing 'year' from a character to a factor
analysis_avgs$year <- as.factor(analysis_avgs$year)

lmm_reading <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_reading)
lmm_reading
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_reading)

#LMM for writing
lmm_writing <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_writing)
lmm_writing
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_writing)

#LMM for math
lmm_math <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_math)
lmm_math
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_math)
```

#### *Reading*

A linear mixed-effects model (LMM) was conducted to assess the effect of academic year on reading growth scores, with year 2008-2009 serving as a reference or baseline year for comparisons. Academic year was treated as a fixed effect, and student ID was entered as a random intercept to account for repeated measurements. Results suggest that academic year significantly predicted reading scores, *F*(3, 276) = 14.24, *p* \< .001. Students in the 2015-2016 academic year scored significantly higher than the reference year (2008-2009), *B* = 0.91, *SE* = 0.15, *t*(276) = 6.04, *p* \< .001. Similarly, students in 2016-2017 also showed significant growth relative to the baseline, *B* = 0.87, *SE* = 0.16, t(276) = 5.53, *p* \< .001. However, no significant difference was found between the 2023-2024 year and the reference year, *B* = -0.10, *SE* = 0.16, *t*(276) = -0.62, *p* = .533. Refer to the linear mixed model output (model is identical for reading, writing, and math) below for full transparency. Results for reading are shown in Fig. 2.

```{r rq2-model, echo=FALSE}
#PRINT LMM OUTPUT
summary(lmm_reading)
```

```{r rq2-visuals-reading, echo=FALSE}
#Line plot: reading, fig 2
ggplot(analysis_avgs, aes(x = year, y = reading)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 2",
       subtitle = "Reading Score Distribution by Academic Year",
       y = "Reading Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

#### *Writing*

A second, identical linear mixed-effects model was applied to examine writing growth scores. Academic year significantly predicted writing scores, *F* (3, 276) = 14.24, *p* \< .001. Writing scores improved significantly in 2015-2016, *B* = 0.91, *SE* = 0.15, *t* (276) = 6.04, *p* \< .001 and 2016-2017, *B* = 0.87, *SE* = 0.16, *t* (276) = 5.53, *p* \< .001 compared to 2008-2009. The difference between 2023-2024 and the reference year was not significant, *B* = -0.10, *SE* = 0.16, *t* (276) = -0.62, *p* = .533. Results for writing are shown in Fig. 3.

```{r rq2-visuals-writing, echo=FALSE}
#Line plot: writing, fig 3
ggplot(analysis_avgs, aes(x = year, y = writing)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 3",
       subtitle = "Writing Score Distribution by Academic Year",
       y = "Writing Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

#### *Math*

A third, identical linear mixed-effects model was conducted to examine the effect of academic year on math growth scores. Academic year significantly predicted math scores, *F* (3, 276) = 14.24, *p* \< .001. Math scores were significantly higher in the 2015-2016 academic year compared to the reference year (2008-2009), *B* = 0.91, *SE* = 0.15, *t* (276) = 6.04, *p* \< .001. Similarly, students in 2016-2017 scored significantly higher than those in the baseline year, *B* = 0.87, *SE* = 0.16, *t* (276) = 5.53, *p* \< .001. In contrast, students’ math scores in 2023-2024 did not significantly differ from the reference year, *B* = -0.10, *SE* = 0.16, *t* (276) = -0.62, *p* = .533. The low variance value was essentially zero (7.725e-16), between reading, writing, and math for the model which suggests that most of the variability in domain scores can be attributed to the effects of year, rather than between-student effects. Results for math are shown in Fig. 4.

```{r rq2-visuals-math, echo=FALSE}
#Line plot: math, fig 4
ggplot(analysis_avgs, aes(x = year, y = math)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 4",
       subtitle = "Math Score Distribution by Academic Year",
       y = "Math Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

# **Discussion**

The main goal of this research paper is to provide evidence for the effectiveness of the Morningside Model of Generative Instruction for children behind in their educational capabilities. We used four waves of longitudinal data from the Morningside Academy in Seattle, Washington, specifically 2008-2009; 2015-2016; 2016-2017, and 2023-2024. Focusing on multiple waves allowed us to establish a more precise view of the educational process in this institution and account for fluctuations across the years and domains. Our main focus of the analysis was on the macro level of testing used, the pre- and post-test, as this is the participants' most important measurement of growth (Street & Johnson, 2014). The descriptive statistics showed that this model is effective in educational settings and provided evidence that using scientific learning and a task-oriented environment supports the progress of students, similar findings have been documented in previous research (Johnson & Street, 2013). Both reading, writing, and math domain scores are higher compared to the initial ones, those result echoes previous research conducted in the scholarship (Johnson & Street, 2004). Being effective in this setting and providing the promised educational advances is crucial for the funding of the academy, considering the vast dependency on tuition. This model can be sufficient, however, more governmental support might be beneficial in the long term, reducing the threat of possible closure due to a year or two of lower results, which can be based on external factors and not on the quality of education.

Our first research question concerns the growth in the academic domain for the chosen period, we found support that growth scores varied across assessment types. The data showed significantly lower reading growth in comparison to math and writing, which were on similar levels. Special education is beneficial for students with disabilities and clinical disorders and improves their reading, math, and writing (Mayes & Calhoun, 2006). Focusing on math and writing growth, we can conclude that the MMGI model yields a positive effect on students in these institutions, improving their abilities. These findings contradict emerging evidence on the effectiveness of special education that questions the practice (Kvande et al., 2019), or found a negative correlation between special education and reading and math abilities in the case of preschoolers (Sullivan & Field, 2013), and math and language skills (Lekhal, 2018). When considering reading, improvement has been documented (Ehrhardt et al., 2013), contradicting our data. The reason for the limited reading growth in our sample might be the ceiling effect, due to the measurements. Additionally, math and writing are usually domains more likely to require interventions. There are vast differences in how special education is positioned and conducted in different countries and contexts, therefore more research will be needed to expand on these contradictory findings.

Our second research question considers the waves of the data, revealing significant variations across the sample years. There was a substantial improvement in growth in the years 2015 to 2017 in all three domains of interest, while 2023-2024 is more similar to the baseline period of the study (2008-2009). We can explain these results in part with the global events influencing the educational system, such as the economic crisis of 2008 and the COVID-19 pandemic. Research has found that education is suffering in times of crisis (Barakat et al., 2010), resources are limited and children are suffering disproportionately (Shafiq, 2010), especially those in marginalized communities (Lovell & Isaacs, 2008). Similarly, the pandemic had an overall negative effect on the educational system due to the constant changes, uncertainty, digital learning (Pokhrel & Chhetri, 2021), and especially for the children with special needs (Mete Yesil et al., 2022). It is unclear whether the mentioned global crises are the sole reasons for the base and current results to be significantly lower than the ones from the 2015-2017 period, more research will be required to answer this question.

## **Implications**

This study provides additional knowledge to the literature in the field of special education, especially scientifically-based learning tools and practical tasks. The findings can be used to inform practices and promote these educational methods for additional implementation across institutions and countries. The study provides data about the student’s growth and capabilities that can be used in educational conversation, policy discussions, and development and has implications for the funding of such academies. Our findings can be beneficial for governmental and state agencies that are centering their efforts to establish and work on educational practices.

## **Limitations**

One of the limitations of our study is failing to account for individual differences can be beneficial for providing a more detailed picture of the effectiveness of MMGI with the inclusion of comparative data across demographics. Additionally, there were a variety of testing methods used for the macro-level establishment of the student’s growth in the Morningside Academy in Seattle. We decided to choose only one of the used testing methods as a primary, future research can look to create a more comprehensive contribution including all testing methods. Examining the reasons for the students to join this type of school is an additional research question that requires attention, as there are contradicting findings surrounding the effectiveness of these institutions, it is important to investigate the characteristics of the participants, their backgrounds, and lived experiences. Follow-up with participants after their time in the academy and evaluating their progress and integration into the traditional school system is another valuable research inquiry.

\newpage

# **References**

-   Barakat, B., Holler, J., Prettner, K., & Schuster, J. (2010). The impact of the economic crisis on labour and education in Europe (Working Paper 6/2010). Vienna Institute of Demography Working Papers. <https://www.econstor.eu/handle/10419/96975>
-   Ehrhardt, J., Huntington, N., Molino, J., & Barbaresi, W. (2013). Special Education and Later Academic Achievement. Journal of Developmental & Behavioral Pediatrics, 34(2), 111. <https://doi.org/10.1097/DBP.0b013e31827df53f>
-   Johnson, K. R., & Street, E. M. (2013). Response to intervention and precision teaching: Creating synergy in the classroom. Guilford Press.
-   Johnson, P. D. K., & Street, P. D. E. M. (2004). The Morningside Model of Generative Instruction: What It Means to Leave No Child Behind. Cambridge Center for Behavioral Studies.
-   Kvande, M. N., Bjørklund, O., Lydersen, S., Belsky, J., & Wichstrøm, L. (2019). Effects of special education on academic achievement and task motivation: A propensity-score and fixed-effects approach. European Journal of Special Needs Education, 34(4), 409–423. <https://doi.org/10.1080/08856257.2018.1533095>
-   Lekhal, R. (2018). Does special education predict students’ math and language skills? European Journal of Special Needs Education, 33(4), 525–540. <https://doi.org/10.1080/08856257.2017.1373494>
-   Mayes, S. D., & Calhoun, S. L. (2006). Frequency of reading, math, and writing disabilities in children with clinical disorders. Learning and Individual Differences, 16(2), 145–157. <https://doi.org/10.1016/j.lindif.2005.07.004>
-   Mete Yesil, A., Sencan, B., Omercioglu, E., & Ozmert, E. N. (2022). The Impact of the COVID-19 Pandemic on Children With Special Needs: A Descriptive Study. Clinical Pediatrics, 61(2), 141–149. <https://doi.org/10.1177/00099228211050223>
-   Pokhrel, S., & Chhetri, R. (2021). A Literature Review on Impact of COVID-19 Pandemic on Teaching and Learning. Higher Education for the Future, 8(1), 133–141. <https://doi.org/10.1177/2347631120983481>
-   Shafiq, M. N. (2010). The Effect of an Economic Crisis on Educational Outcomes: An Economic Framework and Review of the Evidence (SSRN Scholarly Paper 1709045). Social Science Research Network. <https://papers.ssrn.com/abstract=1709045>
-   Street, E. M., & Johnson, K. (2014). The Sciences of Learning, Instruction, and Assessment as Underpinnings of the Morningside Model of Generative Instruction. Acta de Investigación Psicológica, 4(3), 1773–1793. [https://doi.org/10.1016/S2007-4719(14)70979-2](https://doi.org/10.1016/S2007-4719(14)70979-2){.uri}
-   Sullivan, A. L., & Field, S. (2013). Do preschool special education services make a difference in kindergarten reading and mathematics skills?: A propensity score weighting analysis. Journal of School Psychology, 51(2), 243–260. <https://doi.org/10.1016/j.jsp.2012.12.004>
