---
title: "Longitudinal Analysis of Academic Growth Patterns at Morningside Academy:
  A Five-Year Assessment Study"
author:
- name: Saratessa Palos
- name: Will Aragelov
- name: Marcus Cumberbatch
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
#bibliography: references.bib
#csl: apa.csl
abstract: |
  This study analyzed longitudinal assessment data from Morningside Academy...
---

```{r setup, include=FALSE}
# Include libraries here
library(here)
library(readr)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(car)
library(lme4)
library(lmerTest)
library(emmeans)

knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

# Methods

# Results
```{r read-data, include=FALSE}
here::here()
data <- read.csv("~/Desktop/updated_repo/data/updated.csv")
```

```{r datasets, include=FALSE}
#'analysis_data' (clean version of 'data' with 10 variables, and without years 2009-2010) 

#removing rows for 2009-2010 data
analysis_data <- data %>%
  filter(year != "2009-2010")

#removing columns WJID_growth to Math_growth
columns_to_remove <- which(names(analysis_data) == "WJ_LWID_growth"):which(names(analysis_data) == "Math_growth")
analysis_data <- analysis_data[, -columns_to_remove]

#Shifting 'ID' column to left of 'year'
analysis_data <- analysis_data[, c("id", "year", setdiff(names(analysis_data), c("id", "year")))]
```

```{r analysis-avgs, include=FALSE}
#Adding merged means columns (across academic domain measures) to 'data2' frame, for secondary research question
analysis_avgs <- analysis_data

#Reading
analysis_avgs$reading <- (analysis_avgs$RC_growth + analysis_avgs$LWID_growth + analysis_avgs$RF_growth) / 3
analysis_avgs$reading <- round(analysis_avgs$reading, 2)

#Writing
analysis_avgs$writing <- (analysis_avgs$WA_growth + analysis_avgs$WF_growth + analysis_avgs$WS_growth) / 3
analysis_avgs$writing <- round(analysis_avgs$writing, 2)

#Math
analysis_avgs$math <- (analysis_avgs$CALC_growth + analysis_avgs$MF_growth) / 2
analysis_avgs$math <- round(analysis_avgs$math, 2)

#Writing .csv file for data frame
# write_csv(averages_data2, here::here("Desktop", "REORG'D FINAL", "data", "averages_data2.csv"))
```

```{r grand-means, include=FALSE}
#Obtaining grand means for reading, writing, and math measures
mean_reading <- mean(analysis_avgs$reading, na.rm = TRUE)
mean_reading <- round(mean_reading, 2)
mean_reading

mean_writing <- mean(analysis_avgs$writing, na.rm = TRUE)
mean_writing <- round(mean_writing, 2)

mean_math <- mean(analysis_avgs$math, na.rm = TRUE)
mean_math <- round(mean_math, 2)
```

```{r domain-avgs, include=FALSE}
#Obtaining academic domain averages by year
#Reading
reading_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_reading = round(mean(reading, na.rm = TRUE), 2))
reading_by_year

#Writing
writing_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_writing = round(mean(writing, na.rm = TRUE), 2))
writing_by_year

#Math
math_by_year <- analysis_avgs %>% 
  group_by(year) %>% 
  summarize(mean_math = round(mean(math, na.rm = TRUE), 2))
math_by_year

#Merged table for academic growth averages, by year
merged_means <- left_join(reading_by_year, writing_by_year, math_by_year, by = "year")
merged_means <- left_join(merged_means, math_by_year, by = "year")
merged_means
```

Our analytic strategy first involved obtaining aggregate scores of different growth measure types across participants. Reading comprehension, letter-word identification, and reading fluency measures were coalesced into composite scores. Next, we repeated this process for writing, by combining writing fluency, writing samples, and word attack growth scores. Finally, we combined the two assessment measures from mathematics, CALC as well as math fluency, into averaged scores. We then obtained grand means for each of the three academic domains (`Reading` , `Writing` , and `Math`). Refer to table 1 for a breakdown of these means.

```{r avgs-table, echo=FALSE}
#Kable table of merged means
merged_means %>%
  kbl(
    col.names = c("Year", "Mean Reading", "Mean Writing", "Mean Math"),
    caption = "Table 1: Average Growth by Academic Domain",
    booktabs = TRUE
  ) %>%
  kable_styling(
    full_width = FALSE, 
    position = "center"
  ) %>%
  add_header_above(c(" " = 1, "Academic Growth Averages" = 3)) %>%
  footnote(
    general = "Note: NaN indicates missing data for that academic year.",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```

In addition to means analyses, we conducted statistical tests to observe mean comparisons of academic domains, on aggregate. These tests allowed us to account for non-unique participant identification numbers within the current data, and further enabled us to observe potential differences in growth patterns across various academic years.

### Research Question 1

#### *Analytic Strategy*
A repeated measures analysis of variance (ANOVA) was conducted via the `aov` function in R to quantify patterns of academic growth across different assessment measure types. Individual measure types were aggregated by domain across participants to create three, composite academic domains of math, reading, and writing.. The repeated measures approach was utilized because growth scores were measured several times for each student across several assessment types within a given year (as well as during multiple years). Thus, we treated `assessment_type` as a within-subject factor. Since the data included non-unique student IDs, the model accounted for repeated measures per student. Post-hoc pairwise comparisons with Bonferroni correction were conducted to determine specific differences between the three assessment types, to account for uneven sample sizes within `year`.

#### *Results*
```{r rq1-results, include=FALSE}
# Repeated Measures ANOVA: testing
# Transforming data to long format
analysis_long <- analysis_avgs %>% 
  pivot_longer(cols = c(reading, writing, math),
               names_to = "Assessment_Type",
               values_to = "Growth_Score")

#Removing extraneous columns
analysis_long <- analysis_long %>% 
  select(-c(RC_growth:MF_growth))

#Renaming measures
analysis_long <- analysis_long %>%
  mutate(
    Assessment_Type = case_when(
      Assessment_Type == "math" ~ "Math",
      Assessment_Type == "reading" ~ "Reading",
      Assessment_Type == "writing" ~ "Writing",
      TRUE ~ Assessment_Type
    )
  )

# Variable Structure Check
str(analysis_long)

# Changing variables from characters to factors
analysis_long$id <- as.factor(analysis_long$id)
analysis_long$Assessment_Type <- as.factor(analysis_long$Assessment_Type)
analysis_long$year <- as.factor(analysis_long$year)

# Running a repeated Measures ANOVA to analyze RQ1a
aov_analysis <- aov(Growth_Score ~ Assessment_Type + year + Error(id/(Assessment_Type * year)), 
                  data = analysis_long)

#Transforming data into wide format
analysis_wide <- analysis_long %>% 
  pivot_wider(names_from = Assessment_Type, values_from = Growth_Score)
```

```{r inline-anova, include=FALSE}
# For inline code:
f1 <- 5.81
df1 <- 2
df2 <- 259
pval1 <- .003

f2 <- 9.27
df3 <- 3
df4 <- 259
pval2 <- .001

f3 <- 0.62
df5 <- 1
df6 <- 402
pval3 <- .43

# *F*(`r df1`, `r df2`) = `r round(f1, 2)`, *p* < `r ifelse(p_val1 < 0.001, ".001", format(p_val1, digits=3))`

# *F*(`r df3`, `r df4`) = `r round(f2, 2)`, *p* < `r ifelse(p_val2 < 0.001, ".001", format(p_val2, digits=3))`

# *F*(`r df5`, `r df6`) = `r round(f3, 2)`, *p* < `r ifelse(p_val3 < 0.001, ".001", format(p_val3, digits=3))`
```

A repeated measures ANOVA was conducted to examine the effect of Assessment Type (reading, writing, and math) on academic growth scores. Results showed a significant main effect of `assessment_type` , *F*(`r df1`, `r df2`) = `r round(f1, 2)`, *p* < `r ifelse(pval1 < 0.001, ".001", format(pval1, digits=3))`, indicating that growth scores varied across assessment types. Additionally, there was a significant main effect of `year` , *F*(`r df3`, `r df4`) = `r round(f2, 2)`, *p* < `r ifelse(pval2 < 0.001, ".001", format(pval2, digits=3))`, suggesting that academic growth differed across academic years. However, the interaction between `assessment_type` and `year` was not significant, *F*(`r df5`, `r df6`) = `r round(f3, 2)`, *p* < `r ifelse(pval3 < 0.001, ".001", format(pval3, digits=3))`, indicating that growth patterns across assessments were consistent over time.
 
Post-hoc pairwise comparison tests using a Bonferroni correction revealed that growth in `Reading` was significantly lower than in `Math` (*M*difference = -0.92, *p* = 0.015) and `Writing` (*M*difference = -1.10, *p* = 0.003). No significant differences emerged between `Math` and `Writing` (*p* = 1.00), however. These comparisons suggest that students experienced the least academic growth in reading, while growth in math and writing was statistically similar (refer to the ANOVA model output below for full transparency). Results are shown in Fig. 1.

```{r rq1-output, echo=FALSE}
# ANOVA summary (PRINT)
summary(aov_analysis)

# ANOVA Post Hoc: Bonferroni (uneven groups)...(PRINT)
pairwise.t.test(analysis_long$Growth_Score, analysis_long$Assessment_Type, 
                p.adjust.method = "bonferroni")
```

```{r rq1-visual, echo=FALSE}
#Bar plot
anova_analysis <- analysis_long %>%
  group_by(Assessment_Type) %>%
  summarize(
    Mean_Growth = mean(Growth_Score, na.rm = TRUE),
    SD = sd(Growth_Score, na.rm = TRUE),
    N = n(),
    SE = SD / sqrt(N)
  )

ggplot(anova_analysis, aes(x = Assessment_Type, y = Mean_Growth)) +
  geom_bar(stat = "identity", color = "black", fill = "lightgrey", width = 0.6) +
  geom_errorbar(aes(ymin = Mean_Growth - SE, ymax = Mean_Growth + SE), width = 0.1) +
  labs(
    title = "Fig. 1",
    subtitle = "Academic Growth by Assessment Type",
    x = "Assessment Type",
    y = "Mean Growth Score"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

### Research Question 2

#### *Analytic Strategy*
To examine how academic growth varied across different academic years, a linear mixed-effects model (LMM) was conducted, using the `lmer` function within R, in addition to `car` , `lme4` , and `lmertest` for relevant statistics of significance in the output. Academic `year` was utilized as a fixed effect, with student ID included as a random effect to account for repeated measurements, due to certain students appearing in the data set multiple times across years. This method allowed us to model within-student variability while estimating the average effect of academic year on academic growth in reading, writing, and math. The LMM was fit using restricted maximum likelihood (REML) for unbiased estimates of fixed and random effects.

### Results
```{r rq2-lmm, include=FALSE}
#LMM for reading (PRINT MODEL OUTPUTS)
#LMM
#Checking variable structure
str(analysis_avgs)

#Changing 'year' from a character to a factor
analysis_avgs$year <- as.factor(analysis_avgs$year)

lmm_reading <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_reading)
lmm_reading
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_reading)

#LMM for writing
lmm_writing <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_writing)
lmm_writing
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_writing)

#LMM for math
lmm_math <- lmer(reading ~ year + (1 | id), data = analysis_avgs)
summary(lmm_math)
lmm_math
#Model Assumptions
# par(mfrow = c(2, 2))
# plot(lmm_math)
```

#### *Reading*
A linear mixed-effects model (LMM) was conducted to assess the effect of academic year on reading growth scores, with year 2008-2009 serving as a reference or baseline year for comparisons. Academic year was treated as a fixed effect, and student ID was entered as a random intercept to account for repeated measurements. Results suggest that academic year significantly predicted reading scores, *F*(3, 276) = 14.24, *p* < .001. Students in the 2015-2016 academic year scored significantly higher than the reference year (2008-2009), *B* = 0.91, *SE* = 0.15, *t*(276) = 6.04, *p* < .001. Similarly, students in 2016-2017 also showed significant growth relative to the baseline, *B* = 0.87, *SE* = 0.16, t(276) = 5.53, *p* < .001. However, no significant difference was found between the 2023-2024 year and the reference year, *B* = -0.10, *SE* = 0.16, *t*(276) = -0.62, *p* = .533. Refer to the linear mixed model output (model is identical for reading, writing, and math) below for full transparency. Results for reading are shown in Fig. 2.

```{r rq2-model, echo=FALSE}
#PRINT LMM OUTPUT
summary(lmm_reading)
```

```{r rq2-visuals-reading, echo=FALSE}
#Line plot: reading, fig 2
ggplot(analysis_avgs, aes(x = year, y = reading)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 2",
       subtitle = "Reading Score Distribution by Academic Year",
       y = "Reading Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

#### *Writing*
A second, identical linear mixed-effects model was applied to examine writing growth scores. Academic year significantly predicted writing scores, *F* (3, 276) = 14.24, *p* < .001. Writing scores improved significantly in 2015-2016, *B* = 0.91, *SE* = 0.15, *t* (276) = 6.04, *p* < .001 and 2016-2017, *B* = 0.87, *SE* = 0.16, *t* (276) = 5.53, *p* < .001 compared to 2008-2009. The difference between 2023-2024 and the reference year was not significant, *B* = -0.10, *SE* = 0.16, *t* (276) = -0.62, *p* = .533. Results for writing are shown in Fig. 3.

```{r rq2-visuals-writing, echo=FALSE}
#Line plot: writing, fig 3
ggplot(analysis_avgs, aes(x = year, y = writing)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 3",
       subtitle = "Writing Score Distribution by Academic Year",
       y = "Writing Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```

#### *Math*
A third, identical linear mixed-effects model was conducted to examine the effect of academic year on math growth scores. Academic year significantly predicted math scores, *F* (3, 276) = 14.24, *p* < .001. Math scores were significantly higher in the 2015-2016 academic year compared to the reference year (2008-2009), *B* = 0.91, *SE* = 0.15, *t* (276) = 6.04, *p* < .001. Similarly, students in 2016-2017 scored significantly higher than those in the baseline year, *B* = 0.87, *SE* = 0.16, *t* (276) = 5.53, *p* < .001. In contrast, studentsâ€™ math scores in 2023-2024 did not significantly differ from the reference year, *B* = -0.10, *SE* = 0.16, *t* (276) = -0.62, *p* = .533. The low variance value was essentially zero (7.725e-16), between reading, writing, and math for the model which suggests that most of the variability in domain scores can be attributed to the effects of year, rather than between-student effects. Results for math are shown in Fig. 4.

```{r rq2-visuals-math, echo=FALSE}
#Line plot: math, fig 4
ggplot(analysis_avgs, aes(x = year, y = math)) +
  geom_violin(fill = "lightgrey", color = "black", trim = FALSE) +
  stat_summary(fun = median, geom = "point", shape = 19, size = 3, color = "black") + 
  #stat_summary(fun = median, geom = "line", aes(group = 1), color = "black", linetype = "solid") +
  labs(title = "Fig. 4",
       subtitle = "Math Score Distribution by Academic Year",
       y = "Math Score", 
       x = "Academic Year") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(face = "bold"), 
    axis.title.x = element_text(face = "bold", size = 12), 
    axis.title.y = element_text(face = "bold", size = 12), 
    plot.title = element_text(face = "bold", size = 18, hjust = 0), 
    plot.subtitle = element_text(face = "italic", size = 12, hjust = 0),
    )
```




# Discussion

# Conclusion

# References
